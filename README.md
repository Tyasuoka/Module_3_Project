# To Churn or No to Churn

**The Project:**
  For our module 3 project we had a choice of three different data frames that would exercise our knowledge of classification problems when dealing with real world data.


**The Goal:**
  The focus of this project was to build a classification model that is able the predict on the set target of a data frame. Our business problem was binary so our target wasn't hard to distinguish from the rest of the data. 


**The Problem:**
  As the world continues to expand, so does our need to communicate. This is why telecommunication companies are so important, they keep us in touch with one another. With the amount of options your companies offer when it comes to communication, a loyal customer is bond to form amongst the customers you cater to. But that is just an assumption! What if you could tell if a particular customer would churn or not? What if you could predict the loyalty of the people you provide services for? Well my telecom friend you are in luck!!


**The Solution:**
  The top two things when it comes to a business are both making saving money & making money. So if we could tell you who amongst your customer roster will discontinue their services with you, you could create a focus group for that newly discovered roster. From there you could work on potential loyal customers who may just need a little more love when it comes to their services. But before you show them some love you need to find out who needs it first, and that is where our model comes in and predicts who will churn on you. 


**The Process**

The plan of attack was to do the following:
1. Explore & Clean data 
2. Transform data 
3. Model the data

The three steps we are all familiar with when it comes to a new data set! Before we touched the data, we used what we have learned so ofar about binary classification problems to choose how we would measure the performance of our model. 


*The Data:*
   The data was provided by the Kaggle member david_becks which consisted of collected data on churns in telecommunication data.
  
 
*The Metrics:*
  For the metrics we chose Accuracy Score and the ROC AUC Score, you can find more information on the scoring metrics in the resource section located at the bottom of the READ.me.


*The Baseline Model:*
  The baseline model we chose to go with was the Random Forest Classifier because it handles imbalance and muticollineararity in the data. In the reprosity guide located at the bottom of the READ.me you will find the notebook that contains the cleaning, transforming, and exploring of the data, as well as the baseline model, along with a Descion Tree Classifier model. 

*Logistic Regression*

*Gradient Boost & AdaBoost*



# Conclusion

# Future Recommendations

# Repository Guide


# Team Members 

1. **Boi Moriba:**  https://github.com/bmor2552  

2. **Takehiro Yasuoka:** https://github.com/Tyasuoka


# Resources

**The Data:**
  Below is the direct link to our data source.
  
  https://www.kaggle.com/becksddf/churn-in-telecoms-dataset


**The Metrics:**
  The links below will speak more on ROC_AUC and Accuracy Scores.
  
  
  **Models:** Below you will find model documentation
  
  *Random Forest:* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html
  
  *Decision Tree:*  https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html
  
  *Logistic Regression:* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html 
  
  *Gradient Boost:* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html
  
  *AdaBoost:* https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html
  
 
 # Human Resources 
  
  Lindsey Berlin DS 02-17-2020 Coach
  
  Bryan Arnold DS 02-17-2020 Lead Instructor
  
  
